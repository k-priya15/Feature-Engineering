{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering Assignment"
      ],
      "metadata": {
        "id": "urts2IP6SAJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Assignment Questions"
      ],
      "metadata": {
        "id": "hm3dLKpUm-sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1) What is a parameter?\n",
        "\n",
        "- A parameter is a measurable characteristic of a population that is typically unknown and is estimated using data. In the context of statistics, parameters refer to values like the population mean (μ), standard deviation (σ), or proportion (p). In Machine Learning, a parameter usually refers to the internal variables of a model that are learned from training data, such as weights in linear regression or coefficients in logistic regression. These parameters are optimized to minimize the model’s loss function and improve predictive accuracy."
      ],
      "metadata": {
        "id": "75eLw_m0Bwns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kVBKid3GB67f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2) What is correlation? What does negative correlation mean?\n",
        "\n",
        "- Correlation is a statistical measure that expresses the extent to which two variables move in relation to each other. It is quantified using the correlation coefficient, which ranges from -1 to 1. A positive correlation means that as one variable increases, the other also increases. A negative correlation means that as one variable increases, the other decreases. For example, if the temperature increases and the sale of jackets decreases, they show a negative correlation. A correlation close to 0 means there's little or no linear relationship.\n",
        "\n"
      ],
      "metadata": {
        "id": "KcsN9iDhB7l-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7StWvo7rCAll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3) Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "- Machine Learning is a branch of Artificial Intelligence that enables systems to learn patterns from data and make predictions or decisions without being explicitly programmed. The main components of Machine Learning include:\n",
        "\n",
        "Data: The input used to train and evaluate models.\n",
        "\n",
        "Model: A mathematical representation that learns patterns from the data.\n",
        "\n",
        "Loss function: Measures the error between predictions and actual values.\n",
        "\n",
        "Optimization algorithm (optimizer): Minimizes the loss function by adjusting model parameters.\n",
        "\n",
        "Evaluation metrics: Used to assess the performance of the model."
      ],
      "metadata": {
        "id": "Q-qjcZRzCBer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1yuNzPbVCFvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4) How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "- The loss value quantifies the difference between the predicted output of a model and the actual target value. A smaller loss indicates that the model's predictions are close to the actual values, suggesting a better model. Conversely, a high loss implies poor predictions and that the model needs improvement. The loss function is optimized during training, and its value helps guide updates to the model parameters. Monitoring the loss over time helps detect issues like underfitting or overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "y9qnYVAlCGfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4l4rbwsnCLCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5) What are continuous and categorical variables?\n",
        "\n",
        "- Continuous variables are numerical variables that can take an infinite number of values within a given range. Examples include height, temperature, and weight. They are measured and can be fractional.\n",
        "Categorical variables, on the other hand, represent categories or labels and have a limited number of distinct values. They can be nominal (no natural order, like color) or ordinal (with order, like ratings: low, medium, high). Handling these types of variables properly is crucial for effective model training.\n",
        "\n"
      ],
      "metadata": {
        "id": "TAJFm4-5CLxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vq57tuvpCPm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6) How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "- Categorical variables need to be converted into numerical values before being fed into machine learning models. Common techniques include:\n",
        "\n",
        "Label Encoding: Assigns an integer to each category (e.g., red=0, blue=1).\n",
        "\n",
        "One-Hot Encoding: Creates a binary column for each category and assigns 1/0 based on presence.\n",
        "\n",
        "Ordinal Encoding: Assigns ordered integers to categories with a logical order.\n",
        "\n",
        "Binary Encoding or Target Encoding: Used when there are many categories.\n",
        "Choosing the right technique depends on the nature of the data and the model being used.\n",
        "\n"
      ],
      "metadata": {
        "id": "xOnf4c5GCQRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "A35dyxRXCTyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7) What do you mean by training and testing a dataset?\n",
        "\n",
        "- Training and testing a dataset involves dividing your data into two subsets. The training set is used to train the machine learning model, meaning it learns the underlying patterns and relationships in the data. The testing set is used to evaluate the model's performance on unseen data to ensure that it generalizes well. A good practice is to split the data typically in an 80:20 or 70:30 ratio. This separation helps in preventing overfitting and gives a realistic estimate of model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "pE3obfYwCUWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AjcFJnWxCYC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8) What is sklearn.preprocessing?\n",
        "\n",
        "- sklearn.preprocessing is a module in Scikit-learn that provides various utility functions and classes for data preprocessing. These functions help prepare raw data into a format suitable for model training.\n",
        "\n",
        "Common tasks include:\n",
        "\n",
        "Scaling features (e.g., StandardScaler, MinMaxScaler),\n",
        "\n",
        "Encoding categorical variables (e.g., LabelEncoder, OneHotEncoder),\n",
        "\n",
        "Handling missing values, and\n",
        "\n",
        "Polynomial transformations.\n",
        "Proper preprocessing ensures that features are on similar scales and that categorical variables are appropriately represented for the algorithm to learn effectively."
      ],
      "metadata": {
        "id": "Ktu5-rhFCY-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u70wTZfnCgQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9) What is a Test set?\n",
        "\n",
        "- A test set is a portion of the dataset that is separated from the training data and is used solely to evaluate the final performance of a machine learning model. The model never sees the test set during training, ensuring an unbiased assessment of how well it can generalize to new, unseen data. The test set helps detect overfitting and gives a realistic picture of model accuracy, precision, recall, and other performance metrics."
      ],
      "metadata": {
        "id": "EPxcVpFoCg4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HldrPJ21CkwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10) How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "- Data can be split using the train_test_split() function from sklearn.model_selection. It randomly splits data into training and testing sets. The approach to a Machine Learning problem typically involves:\n",
        "\n",
        "Understanding the problem and data.\n",
        "\n",
        "Preprocessing and cleaning the data.\n",
        "\n",
        "Performing Exploratory Data Analysis (EDA).\n",
        "\n",
        "Feature engineering and encoding.\n",
        "\n",
        "Splitting data.\n",
        "\n",
        "Selecting and training a model.\n",
        "\n",
        "Evaluating and tuning the model.\n",
        "\n",
        "Deploying the model.\n",
        "Each step is crucial for building an effective and accurate model."
      ],
      "metadata": {
        "id": "JR7Pvx4KClUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Cu9gPlUBCq9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11) Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "- Exploratory Data Analysis (EDA) helps us understand the underlying structure and patterns in the data before applying any model.\n",
        "\n",
        "It allows us to:\n",
        "\n",
        "Identify missing or inconsistent data.\n",
        "\n",
        "Understand the distribution of variables.\n",
        "\n",
        "Detect outliers and correlations.\n",
        "\n",
        "Visualize relationships between features.\n",
        "EDA provides insights into feature importance and interactions, helping to choose appropriate preprocessing techniques and model selection. It ensures the data quality and guides decisions throughout the modeling process.\n",
        "\n"
      ],
      "metadata": {
        "id": "zBTdjGW-Crii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q1Hsk8--CxaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12) What is correlation?\n",
        "\n",
        "Correlation measures the strength and direction of a linear relationship between two variables. It is expressed as a coefficient ranging from -1 to 1:\n",
        "\n",
        "1 indicates perfect positive correlation.\n",
        "\n",
        "0 means no correlation.\n",
        "\n",
        "-1 shows perfect negative correlation.\n",
        "Understanding correlation is essential in feature selection, as highly correlated features can affect model performance. It’s important to visualize and quantify correlation to ensure model accuracy and avoid multicollinearity."
      ],
      "metadata": {
        "id": "wbdHLqDwCyEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3aP1ANQLC26y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13) What does negative correlation mean?\n",
        "\n",
        "- Negative correlation means that when one variable increases, the other tends to decrease. For example, if the number of hours watching TV increases, academic performance might decrease. This is reflected by a correlation coefficient that lies between -1 and 0. A value closer to -1 implies a strong negative relationship. Negative correlations are useful in regression modeling, feature selection, and understanding inverse relationships in data.\n",
        "\n"
      ],
      "metadata": {
        "id": "UCmS55SMC3fd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ghPJi0DmC73_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14) How can you find correlation between variables in Python?\n",
        "\n",
        "- Correlation can be computed using the .corr() method in Pandas, which calculates the Pearson correlation coefficient by default. To visualize correlation, seaborn.heatmap() can be used.\n",
        "\n",
        "Example:\n",
        "\n",
        "df.corr()\n",
        "sns.heatmap(df.corr(), annot=True)\n",
        "You can also use other methods like Spearman or Kendall for non-linear relationships. This analysis helps identify multicollinearity and decide which features to keep or drop before training a model.\n"
      ],
      "metadata": {
        "id": "MTWN3QHTC86I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ntJoJtGlDEd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15) What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "- Causation implies that one variable directly affects another, whereas correlation only indicates that two variables move together. For example, ice cream sales and drowning incidents may be positively correlated, but one does not cause the other. The real cause may be a third variable like hot weather. Correlation does not imply causation; establishing causality requires controlled experiments or domain knowledge. Understanding this distinction is critical to avoid drawing incorrect conclusions from data.\n",
        "\n"
      ],
      "metadata": {
        "id": "O7vyy5IPDFFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5Fz-bH5dDI8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16) What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "- An optimizer in Machine Learning is an algorithm used to adjust the model parameters (like weights) to minimize the loss function. Common optimizers include:\n",
        "\n",
        "Gradient Descent: Updates parameters in the direction of the negative gradient.\n",
        "\n",
        "Stochastic Gradient Descent (SGD): Uses a single data point per iteration.\n",
        "\n",
        "Adam: Combines momentum and adaptive learning rates for faster convergence.\n",
        "\n",
        "RMSprop: Adjusts learning rate based on recent gradients.\n",
        "Example: In TensorFlow or PyTorch, you might use optimizer = Adam() to update weights during training. Choosing the right optimizer impacts training speed and model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "1BkjVMbcDJhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "41czEsNwDNL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17) What is sklearn.linear_model?\n",
        "\n",
        "- sklearn.linear_model is a sub-module in Scikit-learn that provides various linear models for regression and classification tasks.\n",
        "\n",
        "Some of the commonly used models are:\n",
        "\n",
        "LinearRegression\n",
        "\n",
        "LogisticRegression\n",
        "\n",
        "Ridge, Lasso, and ElasticNet\n",
        "These models are useful when there is a linear relationship between input features and the target variable. The module allows easy training, evaluation, and prediction, making it a core part of any ML workflow.\n",
        "\n"
      ],
      "metadata": {
        "id": "-GVBiiz-DN0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wZIl8u2BDSjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18) What does model.fit() do? What arguments must be given?\n",
        "\n",
        "- model.fit() trains a machine learning model using the provided data.\n",
        "\n",
        "It takes in two main arguments:\n",
        "\n",
        "X (features or input data)\n",
        "\n",
        "y (target or labels)\n",
        "The function estimates the model parameters (like weights) that minimize the loss function. It also stores the learned parameters for later predictions. It’s a fundamental step in model training across all Scikit-learn models.\n",
        "\n"
      ],
      "metadata": {
        "id": "uW1Tx2SnDTJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "G1lnLDaADYwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19) What does model.predict() do? What arguments must be given?\n",
        "\n",
        "- model.predict() uses the learned parameters from training to predict target values for new or unseen data. It requires only the feature data X as input. The function returns predicted values that can be compared against actual values to evaluate the model’s performance. It’s typically used after model.fit() during model testing or deployment.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mc3RDlQRDZZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUmBMqu9DepJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20) What are continuous and categorical variables?\n",
        "\n",
        "- Continuous variables are numerical variables that can take an infinite number of values within a given range. Examples include height, temperature, and weight. They are measured and can be fractional.\n",
        "Categorical variables, on the other hand, represent categories or labels and have a limited number of distinct values. They can be nominal (no natural order, like color) or ordinal (with order, like ratings: low, medium, high). Handling these types of variables properly is crucial for effective model training.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T0Bts-HEDfbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ygh0jW-PDq4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21) What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "- Feature scaling is a technique to normalize the range of independent variables in a dataset. It ensures that all features contribute equally to the result, especially important for algorithms like SVM, KNN, and Gradient Descent. Without scaling, features with larger magnitudes dominate those with smaller values. Techniques include Min-Max Scaling and Standardization. Feature scaling improves model performance, convergence rate, and overall accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "-2suqvOGDrtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FtdVfnwtDvZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22) How do we perform scaling in Python?\n",
        "\n",
        "- In Python, feature scaling is done using:\n",
        "\n",
        "StandardScaler: Scales data to have mean = 0 and std = 1.\n",
        "\n",
        "MinMaxScaler: Scales data to a fixed range, usually [0, 1].\n",
        "\n",
        "Example using Scikit-learn:\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X)\n",
        "This transformation helps models interpret features correctly and perform better.\n",
        "\n"
      ],
      "metadata": {
        "id": "g42N7G-_Dwb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mo3I2cY6D9Yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23) What is sklearn.preprocessing?\n",
        "\n",
        "- sklearn.preprocessing is a module in Scikit-learn that provides various utility functions and classes for data preprocessing. These functions help prepare raw data into a format suitable for model training. Common tasks include:\n",
        "\n",
        "Scaling features (e.g., StandardScaler, MinMaxScaler),\n",
        "\n",
        "Encoding categorical variables (e.g., LabelEncoder, OneHotEncoder),\n",
        "\n",
        "Handling missing values, and\n",
        "\n",
        "Polynomial transformations.\n",
        "Proper preprocessing ensures that features are on similar scales and that categorical variables are appropriately represented for the algorithm to learn effectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "v62IRD7VD-DM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9mnoNEvXEIcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24) How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "- Data can be split using the train_test_split() function from sklearn.model_selection. It randomly splits data into training and testing sets. The approach to a Machine Learning problem typically involves:\n",
        "\n",
        "Understanding the problem and data.\n",
        "\n",
        "Preprocessing and cleaning the data.\n",
        "\n",
        "Performing Exploratory Data Analysis (EDA).\n",
        "\n",
        "Feature engineering and encoding.\n",
        "\n",
        "Splitting data.\n",
        "\n",
        "Selecting and training a model.\n",
        "\n",
        "Evaluating and tuning the model.\n",
        "\n",
        "Deploying the model.\n",
        "Each step is crucial for building an effective and accurate model.\n",
        "\n"
      ],
      "metadata": {
        "id": "utPs3mwdEJ3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "J4gUXAoIETO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25) Explain data encoding?\n",
        "\n",
        "Data encoding is the process of converting categorical values into numerical format for machine learning models. This is necessary because most models can’t handle non-numeric data.\n",
        "\n",
        "Common encoding techniques include:\n",
        "\n",
        "Label Encoding: Converts each label into a unique integer.\n",
        "\n",
        "One-Hot Encoding: Creates binary columns for each category.\n",
        "\n",
        "Ordinal Encoding: Preserves order among categories.\n",
        "Proper encoding helps models understand the categorical data correctly and avoids bias due to artificial ordering.\n",
        "\n"
      ],
      "metadata": {
        "id": "MW_9UltkEUML"
      }
    }
  ]
}